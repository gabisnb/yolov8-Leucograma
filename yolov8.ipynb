{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade git+https://github.com/keras-team/keras-cv -q --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import keras_cv\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 5\n",
    "GLOBAL_CLIPNORM = 10.0\n",
    "IMAGE_WIDTH = 3000\n",
    "IMAGE_HEIGHT = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = [\n",
    "    \"neutrofilo\",\n",
    "    \"linfocito\",\n",
    "    \"monocito\",\n",
    "    \"bastonete\",\n",
    "    \"metamielocito\",\n",
    "    \"eosinofilo\",\n",
    "]\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "\n",
    "# Path to images and annotations\n",
    "path_images = \"/home/gabriela/projetos/yolov8keras/Malu\"\n",
    "path_annot = \"/home/gabriela/projetos/yolov8keras/Malu/annotations\"\n",
    "\n",
    "path_test_img = \"/home/gabriela/projetos/yolov8keras/test\"\n",
    "path_test_annot = \"/home/gabriela/projetos/yolov8keras/test/annotations\"\n",
    "\n",
    "# Get all XML file paths in path_annot and sort them\n",
    "xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_annot, file_name)\n",
    "        for file_name in os.listdir(path_annot)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# xml_test_files = sorted(\n",
    "#     [\n",
    "#         os.path.join(path_test_annot, file_name)\n",
    "#         for file_name in os.listdir(path_test_annot)\n",
    "#         if file_name.endswith(\".xml\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Get all JPEG image file paths in path_images and sort them\n",
    "jpg_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_images, file_name)\n",
    "        for file_name in os.listdir(path_images)\n",
    "        if file_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# jpg_test_files = sorted(\n",
    "#     [\n",
    "#         os.path.join(path_test_img, file_name)\n",
    "#         for file_name in os.listdir(path_test_img)\n",
    "#         if file_name.endswith(\".jpg\")\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutrofilo'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the objects in the annotation xml file\n",
    "def parse_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = os.path.join(path_images, image_name)\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in root.find(\"objects\"):\n",
    "        cls = obj.tag\n",
    "        print(cls)\n",
    "        classes.append(cls)\n",
    "\n",
    "        if(obj.find(\"bbox\") != None):\n",
    "            bbox = obj.find(\"bbox\")\n",
    "            min_x, min_y, width, height = normalize_bounding_box(float(bbox.find(\"x\").text), float(bbox.find(\"y\").text), float(bbox.find(\"width\").text), float(bbox.find(\"height\").text))\n",
    "            boxes.append([min_x, min_y, width, height])\n",
    "        else:\n",
    "            xCoords = []\n",
    "            yCoords = []\n",
    "            for coord in obj:\n",
    "                if (coord.tag.find(\"x\") != -1):\n",
    "                    xCoords.append(float(coord.text))\n",
    "                elif (coord.tag.find(\"y\") != -1):\n",
    "                    yCoords.append(float(coord.text))\n",
    "\n",
    "            xmin, ymin, width, height = calculate_bounding_box_normalized(xCoords, yCoords)\n",
    "            boxes.append([xmin, ymin, width, height])\n",
    "\n",
    "    class_ids = [\n",
    "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "    return image_path, boxes, class_ids\n",
    "\n",
    "#calculate bounding boxes with points provided\n",
    "def calculate_bounding_box_normalized(xCoords, yCoords):\n",
    "    points = np.array([xCoords, yCoords])\n",
    "\n",
    "    min_x = float(np.min(points[0, :]))\n",
    "    min_x = (min_x/IMAGE_WIDTH)*640\n",
    "    min_y = float(np.min(points[1, :]))\n",
    "    # min_y = ((IMAGE_HEIGHT - min_y)/IMAGE_HEIGHT)*640\n",
    "    min_y = (min_y/IMAGE_HEIGHT)*640\n",
    "\n",
    "    max_x = float(np.max(points[0, :]))\n",
    "    max_x = (max_x/IMAGE_WIDTH)*640\n",
    "    max_y = float(np.max(points[1, :]))\n",
    "    # max_y = ((IMAGE_HEIGHT - max_y)/IMAGE_HEIGHT)*640\n",
    "    max_y = (max_y/IMAGE_HEIGHT)*640\n",
    "\n",
    "    width =  max_x - min_x\n",
    "    height = max_y - min_y\n",
    "\n",
    "    return (min_x, min_y, width, height)\n",
    "\n",
    "def normalize_bounding_box(xmin, ymin, width, height):\n",
    "\n",
    "    min_x = (xmin/IMAGE_WIDTH)*640\n",
    "    # min_y = ((IMAGE_HEIGHT - ymin)/IMAGE_HEIGHT)*640\n",
    "    min_y = (ymin/IMAGE_HEIGHT)*640\n",
    "    width = (width/IMAGE_WIDTH)*640\n",
    "    height = (height/IMAGE_HEIGHT)*640\n",
    "\n",
    "    return (min_x, min_y, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 5874.95it/s]\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "bbox = []\n",
    "classes = []\n",
    "\n",
    "for xml_file in tqdm(xml_files):\n",
    "    image_path, boxes, class_ids = parse_annotation(xml_file)\n",
    "    image_paths.append(image_path)\n",
    "    bbox.append(boxes)\n",
    "    classes.append(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (640, 640), interpolation=cv2.INTER_CUBIC)\n",
    "    return image\n",
    "\n",
    "def load_all_images():\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        image = load_image(path)\n",
    "        images.append(image)\n",
    "        # print(image.shape)\n",
    "    return images\n",
    "\n",
    "\n",
    "images = load_all_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[342.23376645794787,\n",
       " 298.69493955093793,\n",
       " 29.797644815241267,\n",
       " 23.602708263216044]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawBoundingBoxes(imageData, imageOutputPath, bboxes, labels, color):\n",
    "    \"\"\"Draw bounding boxes on an image.\n",
    "    imageData: image data in numpy array format\n",
    "    imageOutputPath: output image file path\n",
    "    inferenceResults: inference results array off object (l,t,w,h)\n",
    "    colorMap: Bounding box color candidates, list of RGB tuples.\n",
    "    \"\"\"\n",
    "    for box in bboxes:\n",
    "        for class_id in labels:\n",
    "            left = int(box[0])\n",
    "            bottom = int(box[1])\n",
    "            right = int(box[0]) + int(box[2])\n",
    "            top = int(box[1]) + int(box[3])\n",
    "            label = class_mapping[int(class_id)]\n",
    "            imgHeight, imgWidth, _ = imageData.shape\n",
    "            thick = int((imgHeight + imgWidth) // 900)\n",
    "            print (left, bottom, imgHeight, imgWidth)\n",
    "            print (label)\n",
    "            cv2.rectangle(imageData,(left, top), (right, bottom), color, thick)\n",
    "            cv2.putText(imageData, label, (left, top - 12), 0, 1e-3 * imgHeight, color, thick//3)\n",
    "    cv2.imwrite(imageOutputPath, imageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "506 217 640 640\n",
      "neutrofilo\n",
      "506 217 640 640\n",
      "eosinofilo\n",
      "506 217 640 640\n",
      "linfocito\n",
      "506 217 640 640\n",
      "neutrofilo\n",
      "281 275 640 640\n",
      "neutrofilo\n",
      "281 275 640 640\n",
      "eosinofilo\n",
      "281 275 640 640\n",
      "linfocito\n",
      "281 275 640 640\n",
      "neutrofilo\n",
      "197 398 640 640\n",
      "neutrofilo\n",
      "197 398 640 640\n",
      "eosinofilo\n",
      "197 398 640 640\n",
      "linfocito\n",
      "197 398 640 640\n",
      "neutrofilo\n",
      "207 241 640 640\n",
      "neutrofilo\n",
      "207 241 640 640\n",
      "eosinofilo\n",
      "207 241 640 640\n",
      "linfocito\n",
      "207 241 640 640\n",
      "neutrofilo\n"
     ]
    }
   ],
   "source": [
    "print(len(images))\n",
    "index = 100\n",
    "drawBoundingBoxes(images[index], '/home/gabriela/projetos/yolov8keras/output/example2.jpg', bbox[index], classes[index], (255, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732571901.230574   13491 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1768 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "# print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragged_bboxes = tf.ragged.constant(bbox, dtype=tf.float32)\n",
    "ragged_classes = tf.ragged.constant(classes, dtype=tf.int64)\n",
    "\n",
    "labels = {\n",
    "    \"boxes\": ragged_bboxes.to_tensor(),\n",
    "    \"classes\": ragged_classes.to_tensor(),\n",
    "}\n",
    "\n",
    "# print(labels[\"boxes\"].type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras_cv.models.YOLOV8Detector(\n",
    "#     num_classes=len(class_mapping),\n",
    "#     bounding_box_format=\"xywh\",\n",
    "#     backbone=keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "#         \"yolo_v8_s_backbone_coco\"\n",
    "#     ),\n",
    "#     fpn_depth=2\n",
    "# )\n",
    "\n",
    "backbone=keras_cv.models.YOLOV8Backbone.from_preset(\"yolo_v8_xs_backbone_coco\")\n",
    "\n",
    "model = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=len(class_mapping),\n",
    "    bounding_box_format=\"xywh\",\n",
    "    backbone=backbone,\n",
    "    fpn_depth=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    global_clipnorm=GLOBAL_CLIPNORM,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    ")\n",
    "\n",
    "# model.compile(\n",
    "#     classification_loss='binary_crossentropy',\n",
    "#     box_loss='ciou',\n",
    "#     optimizer=tf.optimizers.SGD(global_clipnorm=10.0),\n",
    "#     jit_compile=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriela/.local/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732571985.631480   13672 service.cc:148] XLA service 0x7f4478017530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732571985.631832   13672 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-11-25 18:59:44.117772: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732571987.171574   13672 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-25 19:00:12.087275: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1732572023.739838   13672 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 73ms/step - box_loss: 6.5561 - class_loss: 8469.9268 - loss: 8476.4834\n",
      "Epoch 2/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 3.8346 - class_loss: 866.2945 - loss: 870.1291\n",
      "Epoch 3/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 2.2650 - class_loss: 225.7611 - loss: 228.0261\n",
      "Epoch 4/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - box_loss: 1.9970 - class_loss: 79.2277 - loss: 81.2247\n",
      "Epoch 5/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 1.9189 - class_loss: 28.0400 - loss: 29.9589\n",
      "Epoch 6/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 1.8030 - class_loss: 11.4966 - loss: 13.2996\n",
      "Epoch 7/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 1.7582 - class_loss: 4.9486 - loss: 6.7068\n",
      "Epoch 8/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 1.5411 - class_loss: 2.0795 - loss: 3.6207\n",
      "Epoch 9/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-0s\u001b[0m -7430us/step - box_loss: 1.4867 - class_loss: 1.4116 - loss: 2.8983\n",
      "Epoch 10/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 1.2957 - class_loss: 0.9518 - loss: 2.2475\n",
      "Epoch 11/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 1.3232 - class_loss: 0.8557 - loss: 2.1788\n",
      "Epoch 12/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 1.1236 - class_loss: 0.7000 - loss: 1.8237\n",
      "Epoch 13/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 1.1151 - class_loss: 0.6228 - loss: 1.7379\n",
      "Epoch 14/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 0.9668 - class_loss: 0.5427 - loss: 1.5095\n",
      "Epoch 15/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 0.9745 - class_loss: 0.4618 - loss: 1.4363\n",
      "Epoch 16/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.9272 - class_loss: 0.4450 - loss: 1.3722\n",
      "Epoch 17/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 0.8502 - class_loss: 0.3979 - loss: 1.2481\n",
      "Epoch 18/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 0.8550 - class_loss: 0.3646 - loss: 1.2196\n",
      "Epoch 19/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 0.8447 - class_loss: 0.3618 - loss: 1.2065\n",
      "Epoch 20/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.8421 - class_loss: 0.3352 - loss: 1.1773\n",
      "Epoch 21/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - box_loss: 0.8453 - class_loss: 0.3377 - loss: 1.1830\n",
      "Epoch 22/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - box_loss: 0.8398 - class_loss: 0.2874 - loss: 1.1272\n",
      "Epoch 23/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-0s\u001b[0m -7476us/step - box_loss: 0.7437 - class_loss: 0.2961 - loss: 1.0398\n",
      "Epoch 24/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.7371 - class_loss: 0.2669 - loss: 1.0040\n",
      "Epoch 25/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - box_loss: 0.7554 - class_loss: 0.2499 - loss: 1.0053\n",
      "Epoch 26/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.6636 - class_loss: 0.2425 - loss: 0.9062\n",
      "Epoch 27/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.6565 - class_loss: 0.2312 - loss: 0.8877\n",
      "Epoch 28/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.6764 - class_loss: 0.2092 - loss: 0.8856\n",
      "Epoch 29/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.6952 - class_loss: 0.2101 - loss: 0.9053\n",
      "Epoch 30/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.6792 - class_loss: 0.2127 - loss: 0.8919\n",
      "Epoch 31/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - box_loss: 0.7350 - class_loss: 0.1913 - loss: 0.9263\n",
      "Epoch 32/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.6597 - class_loss: 0.2030 - loss: 0.8627\n",
      "Epoch 33/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.6120 - class_loss: 0.2058 - loss: 0.8178\n",
      "Epoch 34/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.6438 - class_loss: 0.1881 - loss: 0.8319\n",
      "Epoch 35/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.6050 - class_loss: 0.1834 - loss: 0.7884\n",
      "Epoch 36/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - box_loss: 0.5738 - class_loss: 0.1736 - loss: 0.7474\n",
      "Epoch 37/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-0s\u001b[0m 73ms/step - box_loss: 0.5995 - class_loss: 0.1645 - loss: 0.7641\n",
      "Epoch 38/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.5697 - class_loss: 0.1743 - loss: 0.7440\n",
      "Epoch 39/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.5883 - class_loss: 0.1639 - loss: 0.7522\n",
      "Epoch 40/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - box_loss: 0.6585 - class_loss: 0.1793 - loss: 0.8377\n",
      "Epoch 41/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.5468 - class_loss: 0.1498 - loss: 0.6966\n",
      "Epoch 42/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.5635 - class_loss: 0.1631 - loss: 0.7265\n",
      "Epoch 43/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.5772 - class_loss: 0.1443 - loss: 0.7215\n",
      "Epoch 44/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.5277 - class_loss: 0.1373 - loss: 0.6650\n",
      "Epoch 45/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.4925 - class_loss: 0.1323 - loss: 0.6248\n",
      "Epoch 46/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.5355 - class_loss: 0.1350 - loss: 0.6705\n",
      "Epoch 47/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.5186 - class_loss: 0.1224 - loss: 0.6411\n",
      "Epoch 48/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - box_loss: 0.4698 - class_loss: 0.1207 - loss: 0.5906\n",
      "Epoch 49/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - box_loss: 0.5022 - class_loss: 0.1222 - loss: 0.6244\n",
      "Epoch 50/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-0s\u001b[0m -6833us/step - box_loss: 0.4593 - class_loss: 0.1081 - loss: 0.5674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f4568d908c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(images, labels, batch_size=4, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
