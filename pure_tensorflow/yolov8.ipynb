{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade git+https://github.com/keras-team/keras-cv -q --break-system-packages\n",
    "# %pip install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriela/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-20 18:11:48.751684: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-20 18:11:49.026384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737407509.127237    1462 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737407509.161428    1462 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-20 18:11:49.424213: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import keras_cv\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 5\n",
    "GLOBAL_CLIPNORM = 10.0\n",
    "IMAGE_WIDTH = 3000\n",
    "IMAGE_HEIGHT = 4000\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "TF_DISABLE_MKL=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = [\n",
    "    \"neutrofilo\",\n",
    "    \"linfocito\",\n",
    "    \"monocito\",\n",
    "    \"bastonete\",\n",
    "    \"metamielocito\",\n",
    "    \"eosinofilo\",\n",
    "]\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "\n",
    "# Path to images and annotations\n",
    "path_images = \"/home/gabriela/projetos/yolov8keras/Bach1\"\n",
    "path_annot = \"/home/gabriela/projetos/yolov8keras/Bach1/annotations\"\n",
    "\n",
    "path_test_img = \"/home/gabriela/projetos/yolov8keras/test\"\n",
    "path_test_annot = \"/home/gabriela/projetos/yolov8keras/test/annotations\"\n",
    "\n",
    "# Get all XML file paths in path_annot and sort them\n",
    "xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_annot, file_name)\n",
    "        for file_name in os.listdir(path_annot)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "xml_test_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_test_annot, file_name)\n",
    "        for file_name in os.listdir(path_test_annot)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get all JPEG image file paths in path_images and sort them\n",
    "jpg_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_images, file_name)\n",
    "        for file_name in os.listdir(path_images)\n",
    "        if file_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "jpg_test_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_test_img, file_name)\n",
    "        for file_name in os.listdir(path_test_img)\n",
    "        if file_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutrofilo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the objects in the annotation xml file\n",
    "def parse_annotation(xml_file, path_img):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = os.path.join(path_img, image_name)\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in root.find(\"objects\"):\n",
    "        cls = obj.tag\n",
    "        # print(cls)\n",
    "        classes.append(cls)\n",
    "\n",
    "        if(obj.find(\"bbox\") != None):\n",
    "            bbox = obj.find(\"bbox\")\n",
    "            min_x, min_y, width, height = normalize_bounding_box(float(bbox.find(\"x\").text), float(bbox.find(\"y\").text), float(bbox.find(\"width\").text), float(bbox.find(\"height\").text))\n",
    "            boxes.append([min_x, min_y, width, height])\n",
    "        else:\n",
    "            xCoords = []\n",
    "            yCoords = []\n",
    "            for coord in obj:\n",
    "                if (coord.tag.find(\"x\") != -1):\n",
    "                    xCoords.append(float(coord.text))\n",
    "                elif (coord.tag.find(\"y\") != -1):\n",
    "                    yCoords.append(float(coord.text))\n",
    "\n",
    "            xmin, ymin, width, height = calculate_bounding_box_normalized(xCoords, yCoords)\n",
    "            boxes.append([xmin, ymin, width, height])\n",
    "\n",
    "    class_ids = [\n",
    "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "    return image_path, boxes, class_ids\n",
    "\n",
    "#calculate bounding boxes with points provided\n",
    "def calculate_bounding_box_normalized(xCoords, yCoords):\n",
    "    points = np.array([xCoords, yCoords])\n",
    "\n",
    "    min_x = float(np.min(points[0, :]))\n",
    "    min_x = (min_x/IMAGE_WIDTH)*640\n",
    "    min_y = float(np.min(points[1, :]))\n",
    "    # min_y = ((IMAGE_HEIGHT - min_y)/IMAGE_HEIGHT)*640\n",
    "    min_y = (min_y/IMAGE_HEIGHT)*640\n",
    "\n",
    "    max_x = float(np.max(points[0, :]))\n",
    "    max_x = (max_x/IMAGE_WIDTH)*640\n",
    "    max_y = float(np.max(points[1, :]))\n",
    "    # max_y = ((IMAGE_HEIGHT - max_y)/IMAGE_HEIGHT)*640\n",
    "    max_y = (max_y/IMAGE_HEIGHT)*640\n",
    "\n",
    "    width =  max_x - min_x\n",
    "    height = max_y - min_y\n",
    "\n",
    "    return (min_x, min_y, width, height)\n",
    "\n",
    "def normalize_bounding_box(xmin, ymin, width, height):\n",
    "\n",
    "    min_x = (xmin/IMAGE_WIDTH)*640\n",
    "    # min_y = ((IMAGE_HEIGHT - ymin)/IMAGE_HEIGHT)*640\n",
    "    min_y = (ymin/IMAGE_HEIGHT)*640\n",
    "    width = (width/IMAGE_WIDTH)*640\n",
    "    height = (height/IMAGE_HEIGHT)*640\n",
    "\n",
    "    return (min_x, min_y, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 1828.25it/s]\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "bbox = []\n",
    "classes = []\n",
    "\n",
    "for xml_file in tqdm(xml_files):\n",
    "    image_path, boxes, class_ids = parse_annotation(xml_file, path_images)\n",
    "    image_paths.append(image_path)\n",
    "    bbox.append(boxes)\n",
    "    classes.append(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (640, 640), interpolation=cv2.INTER_CUBIC)\n",
    "    return image\n",
    "\n",
    "def load_all_images():\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        image = load_image(path)\n",
    "        images.append(image)\n",
    "        # print(image.shape)\n",
    "    return images\n",
    "\n",
    "\n",
    "images = load_all_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1279.80it/s]\n"
     ]
    }
   ],
   "source": [
    "image_test_paths = []\n",
    "bbox_test = []\n",
    "classes_test = []\n",
    "\n",
    "for xml_file in tqdm(xml_test_files):\n",
    "    image_path, boxes, class_ids = parse_annotation(xml_file, path_test_img)\n",
    "    image_test_paths.append(image_path)\n",
    "    bbox_test.append(boxes)\n",
    "    classes_test.append(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (640, 640), interpolation=cv2.INTER_CUBIC)\n",
    "    return image\n",
    "\n",
    "def load_all_images(paths):\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        image = load_image(path)\n",
    "        images.append(image)\n",
    "        # print(image.shape)\n",
    "    return images\n",
    "\n",
    "\n",
    "images = load_all_images(image_paths)\n",
    "\n",
    "test_images = load_all_images(image_test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawBoundingBoxes(imageData, imageOutputPath, bboxes, labels, color):\n",
    "    \"\"\"Draw bounding boxes on an image.\n",
    "    imageData: image data in numpy array format\n",
    "    imageOutputPath: output image file path\n",
    "    inferenceResults: inference results array off object (l,t,w,h)\n",
    "    colorMap: Bounding box color candidates, list of RGB tuples.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for box in bboxes:\n",
    "        left = int(box[0])\n",
    "        bottom = int(box[1])\n",
    "        right = int(box[0]) + int(box[2])\n",
    "        top = int(box[1]) + int(box[3])\n",
    "        label = class_mapping[int(labels[i])]\n",
    "        i+=1\n",
    "        imgHeight, imgWidth, _ = imageData.shape\n",
    "        thick = int((imgHeight + imgWidth) // 900)\n",
    "        print (left, bottom, imgHeight, imgWidth)\n",
    "        print (label)\n",
    "        cv2.rectangle(imageData,(left, top), (right, bottom), color, thick)\n",
    "        cv2.putText(imageData, label, (left, top - 20), 0, 1e-3 * imgHeight, color, thick//3)\n",
    "    cv2.imwrite(imageOutputPath, imageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 311 640 640\n",
      "eosinofilo\n"
     ]
    }
   ],
   "source": [
    "# print(len(images))\n",
    "index = 50\n",
    "drawBoundingBoxes(images[index], '/home/gabriela/projetos/yolov8keras/output/example_Bach1.jpg', bbox[index], classes[index], (0, 0, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737407536.934319    1462 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1768 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "# print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "ragged_bboxes = tf.ragged.constant(bbox, dtype=tf.float32)\n",
    "ragged_classes = tf.ragged.constant(classes, dtype=tf.int64)\n",
    "\n",
    "labels = {\n",
    "    \"boxes\": ragged_bboxes.to_tensor(),\n",
    "    \"classes\": ragged_classes.to_tensor(),\n",
    "}\n",
    "\n",
    "# print(labels[\"boxes\"].type())\n",
    "print(len(labels[\"classes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras_cv.models.YOLOV8Detector(\n",
    "#     num_classes=len(class_mapping),\n",
    "#     bounding_box_format=\"xywh\",\n",
    "#     backbone=keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "#         \"yolo_v8_s_backbone_coco\"\n",
    "#     ),\n",
    "#     fpn_depth=2\n",
    "# )\n",
    "\n",
    "backbone=keras_cv.models.YOLOV8Backbone.from_preset(\"yolo_v8_xs_backbone_coco\")\n",
    "\n",
    "model = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=len(class_mapping),\n",
    "    bounding_box_format=\"xywh\",\n",
    "    backbone=backbone,\n",
    "    fpn_depth=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    global_clipnorm=GLOBAL_CLIPNORM,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels[\"boxes\"], labels[\"classes\"]))\n",
    "\n",
    "# model.compile(\n",
    "#     classification_loss='binary_crossentropy',\n",
    "#     box_loss='ciou',\n",
    "#     optimizer=tf.optimizers.SGD(global_clipnorm=10.0),\n",
    "#     jit_compile=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(640, 640, 3), dtype=float32, numpy=\n",
       " array([[[10.,  5.,  6.],\n",
       "         [ 9.,  4.,  5.],\n",
       "         [ 8.,  3.,  4.],\n",
       "         ...,\n",
       "         [ 4.,  0.,  0.],\n",
       "         [ 7.,  2.,  3.],\n",
       "         [ 7.,  2.,  3.]],\n",
       " \n",
       "        [[12.,  7.,  8.],\n",
       "         [ 7.,  2.,  3.],\n",
       "         [ 5.,  1.,  1.],\n",
       "         ...,\n",
       "         [ 8.,  3.,  4.],\n",
       "         [ 9.,  4.,  5.],\n",
       "         [ 5.,  0.,  1.]],\n",
       " \n",
       "        [[ 4.,  0.,  0.],\n",
       "         [ 7.,  2.,  3.],\n",
       "         [14.,  9., 10.],\n",
       "         ...,\n",
       "         [ 7.,  2.,  3.],\n",
       "         [ 8.,  3.,  4.],\n",
       "         [13.,  8.,  9.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[20., 14., 15.],\n",
       "         [26., 20., 21.],\n",
       "         [26., 21., 22.],\n",
       "         ...,\n",
       "         [ 7.,  2.,  3.],\n",
       "         [ 6.,  1.,  2.],\n",
       "         [ 6.,  1.,  2.]],\n",
       " \n",
       "        [[21., 16., 17.],\n",
       "         [19., 13., 14.],\n",
       "         [26., 20., 21.],\n",
       "         ...,\n",
       "         [10.,  5.,  6.],\n",
       "         [ 8.,  3.,  4.],\n",
       "         [ 8.,  3.,  4.]],\n",
       " \n",
       "        [[28., 23., 24.],\n",
       "         [29., 21., 22.],\n",
       "         [26., 20., 21.],\n",
       "         ...,\n",
       "         [10.,  5.,  6.],\n",
       "         [10.,  5.,  6.],\n",
       "         [ 9.,  4.,  5.]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       " array([[274.25388 , 241.70422 ,  35.532894,  26.089472],\n",
       "        [  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  0.      ,   0.      ,   0.      ,   0.      ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([3, 0, 0, 0, 0])>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model.fit(images, labels, batch_size=4, epochs=50)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_detector.py:525\u001b[0m, in \u001b[0;36mYOLOV8Detector.train_step\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    523\u001b[0m data \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    524\u001b[0m args \u001b[38;5;241m=\u001b[39m args[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 525\u001b[0m x, y \u001b[38;5;241m=\u001b[39m unpack_input(data)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain_step(\u001b[38;5;241m*\u001b[39margs, (x, y))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# model.fit(images, labels, batch_size=4, epochs=50)\n",
    "model.fit(dataset, batch_size=4, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_images)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
